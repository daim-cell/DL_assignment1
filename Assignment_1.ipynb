{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnet:\n",
    "    def __init__(self,neuron_val):\n",
    "        self.nn_architecture = [\n",
    "    {\"input_dim\": 28*28, \"output_dim\": neuron_val, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": neuron_val, \"output_dim\": 10, \"activation\": \"sigmoid\"}]\n",
    "        number_of_layers = len(self.nn_architecture)\n",
    "        self.params_values = {}\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "            self.params_values['W' + str(layer_idx)] = np.random.normal(\n",
    "                mu,sigma,[layer_output_size, layer_input_size]) \n",
    "            self.params_values['b' + str(layer_idx)] = np.random.normal(\n",
    "                mu,sigma,[layer_output_size, 1])\n",
    "            \n",
    "    def params(self):\n",
    "        return self.params_values\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def sigmoid_backward(self, dA, Z):\n",
    "        sig = sigmoid(Z)\n",
    "        return dA * sig * (1 - sig)\n",
    "\n",
    "    def relu_backward(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0;\n",
    "        return dZ;\n",
    "    \n",
    "\n",
    "    \n",
    "    def feedforward(self,X):\n",
    "        self.memory = {}\n",
    "        A_curr = X\n",
    "\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            A_prev = A_curr\n",
    "\n",
    "            activation = layer[\"activation\"]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "            if activation is \"relu\":\n",
    "                A_curr = self.relu(Z_curr)\n",
    "            elif activation is \"sigmoid\":\n",
    "                A_curr = self.sigmoid(Z_curr)\n",
    "\n",
    "            self.memory[\"A\" + str(idx)] = A_prev\n",
    "            self.memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "        return A_curr, self.memory\n",
    "    \n",
    "    def sgd(self,data, epochs, learning_rate, k):\n",
    "        \n",
    "        data = data.sample(k)\n",
    "        self.cost_history = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            Y_hat, cashe = feedforward(X)\n",
    "            cost = get_cost_value(Y_hat, Y)\n",
    "            self.cost_history.append(cost)\n",
    "            self.params_values = update(learning_rate)\n",
    "\n",
    "        return self.params_values, self.cost_history\n",
    "    \n",
    "    def update(self,learning_rate):\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        for layer_idx, layer in enumerate(self.nn_architecture):\n",
    "            self.params_values[\"W\" + str(layer_idx)] -= learning_rate * self.grads_values[\"dW\" + str(layer_idx)]        \n",
    "            self.params_values[\"b\" + str(layer_idx)] -= learning_rate * self.grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "        return params_values\n",
    "    \n",
    "    def train(self):\n",
    "        inp = np.random.randn(28*28)\n",
    "        x,y = self.feedforward(inp)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05229861  0.07917146 -0.08841653 ... -0.02548471  0.08786482\n",
      "  -0.05552382]\n",
      " [-0.02976519  0.10498979  0.07654191 ...  0.08844523 -0.01709367\n",
      "   0.00765424]]\n"
     ]
    }
   ],
   "source": [
    "x = Neuralnet()\n",
    "y = x.params()\n",
    "w,b = x.train()\n",
    "print(y['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
