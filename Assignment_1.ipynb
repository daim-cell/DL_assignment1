{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnet:\n",
    "    def __init__(self,neuron_val, lr, batch_size, epochs):\n",
    "        self.lr, self.batch_size, self.epochs = lr, batch_size, epochs\n",
    "        self.nn_architecture = [\n",
    "    {\"input_dim\": 28*28, \"output_dim\": neuron_val, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": neuron_val, \"output_dim\": 10, \"activation\": \"sigmoid\"}]\n",
    "        number_of_layers = len(self.nn_architecture)\n",
    "        self.params_values = {}\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "            self.params_values['W' + str(layer_idx)] = np.random.normal(\n",
    "                mu,sigma,[layer_output_size, layer_input_size]) \n",
    "            self.params_values['b' + str(layer_idx)] = np.random.normal(\n",
    "                mu,sigma,[layer_output_size, 1])\n",
    " \n",
    "    def params(self):\n",
    "        return self.params_values\n",
    "    \n",
    "    def softmax(self, x):\n",
    "    # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def softmax_backward(self, dA, Z):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "\n",
    "    def relu_backward(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0;\n",
    "        return dZ;\n",
    "    \n",
    "\n",
    "    \n",
    "    def feedforward(self,X):\n",
    "        self.memory = {}\n",
    "        A_curr = X.T\n",
    "\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            A_prev = A_curr\n",
    "            \n",
    "\n",
    "            activation = layer[\"activation\"]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "            if activation is \"relu\":\n",
    "                A_curr = self.relu(Z_curr)\n",
    "            elif activation is \"softmax\":\n",
    "                A_curr = self.softmax(Z_curr)\n",
    "\n",
    "            self.memory[\"A\" + str(idx)] = A_prev\n",
    "            self.memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "            print(A_curr.shape)\n",
    "\n",
    "        return A_curr\n",
    "    \n",
    "    \n",
    "    def backprop(self,Y_hat, Y):\n",
    "        self.grads_values = {}\n",
    "        m = Y.shape[1]\n",
    "        Y = Y.to_numpy().reshape(Y_hat.shape)\n",
    "\n",
    "        dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.nn_architecture))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            activ_function_curr = layer[\"activation\"]\n",
    "\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            A_prev = self.memory[\"A\" + str(layer_idx_prev)]\n",
    "            Z_curr = self.memory[\"Z\" + str(layer_idx_curr)]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx_curr)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx_curr)]\n",
    "            \n",
    "            m = A_prev.shape[1]\n",
    "\n",
    "            if  activ_function_curr is \"relu\":\n",
    "                dZ_curr = self.relu_backward(dA_curr, Z_curr)\n",
    "            else:\n",
    "                dZ_curr = self.softmax_backward(dA_curr, Z_curr)\n",
    "                        \n",
    "            dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "            db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "            dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "            self.grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "            self.grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "            \n",
    "\n",
    "    def sgd(self,X,y):\n",
    "        \n",
    "        self.cost_history = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            Y_hat = self.feedforward(X)\n",
    "            cost = self.get_cost_value(y, Y_hat.T)\n",
    "            self.cost_history.append(cost)\n",
    "            self.update(y,Y_hat)\n",
    "\n",
    "        return self.params_values, self.cost_history\n",
    "    \n",
    "    \n",
    "    def update(self,y,Y_hat):\n",
    "        self.backprop(Y_hat, y)\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            self.params_values[\"W\" + str(layer_idx)] -= self.lr * self.grads_values[\"dW\" + str(layer_idx)]        \n",
    "            self.params_values[\"b\" + str(layer_idx)] -= self.lr * self.grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    \n",
    "    def get_cost_value(self, y, y_hat):\n",
    "        inner = np.power((y - y_hat), 2)\n",
    "        return np.sum(inner) / (2 * len(y))\n",
    "    \n",
    "    def evaluate(self,X):\n",
    "        A_curr = X.T\n",
    "\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_idx = idx + 1\n",
    "            A_prev = A_curr\n",
    "            \n",
    "\n",
    "            activation = layer[\"activation\"]\n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "            if activation is \"relu\":\n",
    "                A_curr = self.relu(Z_curr)\n",
    "            elif activation is \"sigmoid\":\n",
    "                A_curr = self.sigmoid(Z_curr)\n",
    "\n",
    "        return A_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 784), y=(60000, 10)\n",
      "Test: X=(10000, 784), y=(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "trainy = pd.get_dummies(trainy)\n",
    "testy = pd.get_dummies(testy)\n",
    "\n",
    "trainX = trainX.reshape(60000,28*28)\n",
    "testX = testX.reshape(10000, 28*28)\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neuralnet(15,0.1,16,1)\n",
    "y = model.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10000)\n",
      "(15, 10000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to DataFrame, shape must be (10000, 10): given (10000, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-319-10e81dc2b695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-317-5f4c838ead5c>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mY_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cost_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-317-5f4c838ead5c>\u001b[0m in \u001b[0;36mget_cost_value\u001b[1;34m(self, y, y_hat)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_cost_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0minner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_frame_arith_method_with_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis)\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 664\u001b[1;33m                     \u001b[1;34m\"Unable to coerce to DataFrame, shape \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m                     \u001b[1;34mf\"must be {left.shape}: given {right.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to DataFrame, shape must be (10000, 10): given (10000, 15)"
     ]
    }
   ],
   "source": [
    "x= model.sgd(testX,testy)\n",
    "y = model.evaluate(testX)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
